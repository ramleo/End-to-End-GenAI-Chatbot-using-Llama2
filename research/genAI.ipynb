{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DA1041TU\\Downloads\\GenAI_Chatbot\\End-to-End-GenAI-Chatbot-using-Llama2\\genaichatbot\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting an environment variable called PINECONE_API_KEY\n",
    "# os.environ['PINECONE_API_KEY'] = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Loading pdf data\n",
    "def pdf_load(data):\n",
    "  loader = DirectoryLoader(\n",
    "              data,\n",
    "              glob=\"*.pdf\",\n",
    "              loader_cls=PyPDFLoader)\n",
    "  \n",
    "  docs = loader.load()\n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "extracted_data = pdf_load(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting the loaded data\n",
    "def split_text(extracted_data):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "  text_chunks = text_splitter.split_documents(extracted_data)\n",
    "  return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks of pdf file\n",
    "text_chunks = split_text(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created is: 1330\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks created is: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embedding model\n",
    "def download_hf_embeddings():\n",
    "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector embeddings using huggingface \"all-MiniLM-L6-v2\" model\n",
    "# It takes 3 mins to execute\n",
    "embeddings = download_hf_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of query result is: 384\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with the embeddings we just created\n",
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(f\"Length of query result is: {len(query_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting API Key\n",
    "api_key = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting API Key\n",
    "# api_key = os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the pinecone\n",
    "pc = pinecone.Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"genai-chatbot\"\n",
    "chunks_embeddings = PineconeVectorStore.from_texts([chunk.page_content for chunk in text_chunks], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a transformer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chunks_embeddings.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      " [Document(id='9f1acd25-b183-4ab0-befc-b9ab275ff4b1', metadata={}, page_content='The decoding part of the transformer starts with a similar process as the encoding part, where the \\ntarget sequence (output sequence) undergoes input embedding and positional encoding. Let’s under-\\nstand these blocks:\\n• Output embedding (shifted right): For the decoder, the target sequence is “shifted right” by \\none position. This means that at each position, the model tries to predict the token that comes'), Document(id='6299f47f-4374-4498-bfe4-2da8934bc822', metadata={}, page_content='The decoding part of the transformer starts with a similar process as the encoding part, where the \\ntarget sequence (output sequence) undergoes input embedding and positional encoding. Let’s under-\\nstand these blocks:\\n• Output embedding (shifted right): For the decoder, the target sequence is “shifted right” by \\none position. This means that at each position, the model tries to predict the token that comes'), Document(id='5ea19136-5c47-4c73-9909-b709ea477394', metadata={}, page_content='The decoding part of the transformer starts with a similar process as the encoding part, where the \\ntarget sequence (output sequence) undergoes input embedding and positional encoding. Let’s under-\\nstand these blocks:\\n• Output embedding (shifted right): For the decoder, the target sequence is “shifted right” by \\none position. This means that at each position, the model tries to predict the token that comes')]\n"
     ]
    }
   ],
   "source": [
    "print(\"result\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decoding part of the transformer starts with a similar process as the encoding part, where the \n",
      "target sequence (output sequence) undergoes input embedding and positional encoding. Let’s under-\n",
      "stand these blocks:\n",
      "• Output embedding (shifted right): For the decoder, the target sequence is “shifted right” by \n",
      "one position. This means that at each position, the model tries to predict the token that comes\n",
      "The decoding part of the transformer starts with a similar process as the encoding part, where the \n",
      "target sequence (output sequence) undergoes input embedding and positional encoding. Let’s under-\n",
      "stand these blocks:\n",
      "• Output embedding (shifted right): For the decoder, the target sequence is “shifted right” by \n",
      "one position. This means that at each position, the model tries to predict the token that comes\n",
      "The decoding part of the transformer starts with a similar process as the encoding part, where the \n",
      "target sequence (output sequence) undergoes input embedding and positional encoding. Let’s under-\n",
      "stand these blocks:\n",
      "• Output embedding (shifted right): For the decoder, the target sequence is “shifted right” by \n",
      "one position. This means that at each position, the model tries to predict the token that comes\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)):\n",
    "  print(result[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's questions.\n",
    "If you don't know the answer, just say that you don't know the answer, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return helpful answer and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt template\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(\n",
    "  model=\"model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "  model_type=\"llama\",\n",
    "  config={\"max_new_tokens\": 700, \"temperature\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=chunks_embeddings.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalqa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=chunks_embeddings.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x00000210CA1AF7D0>, model='model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 700, 'temperature': 0.7}), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='Context:\\n{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x00000210C788B550>, search_kwargs={'k': 2}))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrievalqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a transformer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes approx. 27 mins to execute in my device, however in colab it takes 7 mins\n",
    "result1 = retrievalqa.invoke({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is a transformer?',\n",
       " 'result': ' A transformer is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It\\'s primarily designed for sequence-to-sequence tasks, such as machine translation, text summarization, and language modeling. The transformer consists of an encoder and a decoder, each composed of multiple identical layers. Each layer in the encoder and decoder consists of self-attention mechanisms, feedforward networks, and layer normalization. The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance, while the feedforward network processes the output of the self-attention mechanism to generate the final output.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A transformer is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It\\'s primarily designed for sequence-to-sequence tasks, such as machine translation, text summarization, and language modeling. The transformer consists of an encoder and a decoder, each composed of multiple identical layers. Each layer in the encoder and decoder consists of self-attention mechanisms, feedforward networks, and layer normalization. The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance, while the feedforward network processes the output of the self-attention mechanism to generate the final output.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A transformer is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It's primarily designed for sequence-to-sequence tasks, such as machine translation, text summarization, and language modeling. The transformer consists of an encoder and a decoder, each composed of multiple identical layers. Each layer in the encoder and decoder consists of self-attention mechanisms, feedforward networks, and layer normalization. The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance, while the feedforward network processes the output of the self-attention mechanism to generate the final output.\n"
     ]
    }
   ],
   "source": [
    "print(result1[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaichatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
